This is the readme of my project:
```
# DriveNetBench

DriveNetBench is a comprehensive benchmarking repository for evaluating neural networks designed for autonomous driving control.


![til](./assets/bench.gif)

## Project Description

DriveNetBench is a benchmarking tool designed to evaluate the performance of neural networks for autonomous driving control. The tool uses a single camera to navigate predefined tracks and provides metrics to assess the accuracy and efficiency of the neural networks. The main features of DriveNetBench include:

- Detection and tracking of the robot using a YOLO model.
- Calculation of path similarity using Dynamic Time Warping (DTW) or Frechet distance.
- Extraction and processing of track images to define keypoints and paths.
- Visualization of the robot's path and track.

## Requirements

- Miniconda or Anaconda

## Installation

To install DriveNetBench, follow these steps:

1. Clone the repository:
   ```bash
   git clone https://github.com/alibustami/DriveNetBench.git
   cd DriveNetBench
   ```
### Unix
2. Create a virtual environment and build the package:
   ```bash
   make create-venv
   ```

### Windows
2. Create a virtual environment and build the package:
   ```
   conda create -n drivenetbench python=3.8
   conda activate drivenetbench
   python -m pip install --upgrade pip
   python -m pip install --upgrade setuptools wheel
   python -m pip install -e .
   pre-commit install
   ```

## Configuration

The `config.yaml` file is the only file you need to modify in the entire repository. This file contains all the necessary configurations for running the benchmark.

### Configuration Parameters

The `config.yaml` file contains the following parameters:
- `Benchmarker` class:
   - `benchmarker.video_path`: The path to the video file to be benchmarked.
   - `benchmarker.detection_model.model_path`: The path to the YOLO model weights.
   - `benchmarker.detection_model.conf_threshold`: The confidence threshold for the YOLO model.
   - `benchmarker.detection_model.shift_ratio`: The ratio to shift the detection centroid to the bottom from the top.
       > if the detection centriod is at the very bottom of the image the shift value will be 0.0 pixels, but if the detection centroid is at the very top of the image the shift value `shift_ratio` * `frame_height` pixels.
   - `benchmarker.track_image_path`: The path to the track image.
   - `benchmarker.reference_track_npy_path`: The path to the reference track numpy file (generated by the [`TrackProcessor`](https://github.com/alibustami/DriveNetBench/blob/7df9007bed8fb125752df4b8950018982b411c0c/drivenetbench/utilities/track_processor.py#L15) class).
   - `benchmarker.path_similarity.method`: The method used to calculate the path similarity. The available methods are `dtw` and `frechet`.
   - `benchmarker.path_similarity.auto_tune`: A boolean value indicating whether to automatically tune the path similarity parameter `clamp_distance` and `distance_baseline`.
   - `benchmarker.path_similarity.clamp_percentage`: The percentage of the path length to be used as the clamp distance.
   - `benchmarker.path_similarity.clamp_distance`: The maximum distance between two points to be considered as a match.
   - `benchmarker.path_similarity.distance_baseline`: The distance baseline for the matching points.
       > if the `auto_tune` is set to `True`, the `clamp_distance` and `distance_baseline` will be automatically tuned.
   - `benchmarker.time.distance_threshold_in_pixels`: The linear distance between the robot's starting point and ending point to be considered as a full rotation.
   - `benchmarker.time.skip_first_x_seconds`: The number of seconds to skip at the beginning of the video.
- `ViewTransform` class:
  - `view_transformer.source_path`: The path to the source image (the image from the camera view).
  - `view_transformer.target_path`: The path to the target image (the image from the digital graph).
- `KeyPointsDefiner` class:
  - `keypoints_definer.source_path`: The path of the image to define the keypoints from.
  - `keypoints_definer.output_name`: The name of the output file to save the keypoints -including the extension-.
  - `keypoints_definer.frame_number`: The frame number to define the keypoints from, if the source is a video. Otherwise, set 1.
  - `keypoints_definer.override_if_exists`: A boolean value indicating whether to override the output file if it already exists.
- `TrackProcessor` class:
  - `track_processor.image_path`: The path to the track image.
  - `track_processor.color_hsv`: The color of the track in HSV format. https://pinetools.com/image-color-picker was used to get the HSV values.
  - `track_processor.output_image_path_export`: The path to save the processed track image, including the file extension.
  - `track_processor.output_npy_path_export`: The path to save the processed track numpy file, including the file extension (.npy).
  - `track_processor.only_offset_the_outer`: A boolean value indicating whether to only offset the outer track.
  - `track_processor.dbscan.eps`: The maximum distance between two samples for one to be considered as in the neighborhood of the other.
  - `track_processor.dbscan.min_samples`: The number of samples in a neighborhood for a point to be considered as a core point.
  - `track_processor.dbscan.cluster_size_threshold`: The minimum number of points in a cluster to be considered as a track.
- General parameters, `actions` objects:
  - `actions.early_stop_after_x_seconds`: The number of seconds to wait before stopping the benchmark, leave empty to disable.
  - `actions.show_live`: A boolean value indicating whether to show the live video feed.
  - `actions.save_live_to_disk`: A boolean value indicating whether to save the live video feed to disk.

## Usage

Keep in mind all parameters and configurations related to the project are stored in the `config.yaml` file.

1. Define the keypoints:
   1.1 Define the keypoints from the digital graph, specify the `keypoints_definer.source_path` in the config.yaml file to the digital track image.
   ```bash
   drivenetbench define-keypoints --config-file config.yaml
   ```

   commands for the `DefineKeyPoints` class:
   1. press `z` to remove the last point.
   2. press `q` to save the points and exit.


   1.2 Change the path of the `keypoints_definer.source_path` in the config.yaml to your source path. Then re-run the previous command.

   > ⚠️ Ensure that points order is the same, otherwise the view transformation will not work correctly.

   > you can run the `notebooks/calculate_transformation_error.ipynb` to check your view transformation. You will end up with a plot showing the transformation error. like the following:
   ![Transformation Error](./assets/transformation_error.png)

   Example of the keypoints definition:
   3. Define the keypoints from the digital graph
      ![Digital Graph](./assets/define-keypoints-track.png)
   4. Define the keypoints from the camera view
      ![Camera View](./assets/define-keypoints-camera.png)

   > See how the keypoints orders are the same.

2. Extract the track from the image:
   ```bash
   drivenetbench extract-track --config-file config.yaml
   ```

   You will end up with:
   - the track as npy file.
   - the track drawn on the image, as shown below:
     ![Track Image](./assets/new_track/annotated_track.jpg)

3. Run the benchmark:
   ```bash
   drivenetbench benchmark --config-file config.yaml
   ```

The results of the benchmark will be saved in the `results` directory. The results include the time taken for the robot to complete a full rotation and the similarity percentage between the robot's path and the reference path.

## Contributing

We welcome contributions to DriveNetBench! If you have any ideas, suggestions, or bug reports, please open an issue on GitHub. If you would like to contribute code, please follow these guidelines:

1. Fork the repository and create a new branch for your feature or bugfix.
2. Write tests for your changes and ensure that all tests pass.
3. Submit a pull request with a clear description of your changes.

## License

DriveNetBench is licensed under the MIT License. See the [LICENSE](LICENSE) file for more information.
```

this is the project tree:
```
$ tree
.
├── DriveNetBench.egg-info
│   ├── PKG-INFO
│   ├── SOURCES.txt
│   ├── dependency_links.txt
│   ├── entry_points.txt
│   ├── requires.txt
│   └── top_level.txt
├── Makefile
├── README.md
├── asdfsad.txt
├── assets
│   ├── NN_Diagram.jpg
│   ├── bench.gif
│   ├── define-keypoints-camera.png
│   ├── define-keypoints-track.png
│   ├── new_track
│   │   ├── annotated_track.jpg
│   │   ├── base.jpg
│   │   ├── driver_1.MOV
│   │   ├── driver_2.MOV
│   │   ├── driver_3.MOV
│   │   ├── driver_4.MOV
│   │   └── track-v2.jpg
│   └── transformation_error.png
├── config.yaml
├── drivenetbench
│   ├── __init__.py
│   ├── __pycache__
│   │   ├── __init__.cpython-310.pyc
│   │   ├── __init__.cpython-311.pyc
│   │   ├── __init__.cpython-38.pyc
│   │   ├── __init__.cpython-39.pyc
│   │   ├── benchmarker.cpython-311.pyc
│   │   ├── benchmarker.cpython-38.pyc
│   │   ├── detector.cpython-38.pyc
│   │   ├── geometry_definer.cpython-38.pyc
│   │   ├── keypoints_definer.cpython-38.pyc
│   │   ├── similarity_calculator.cpython-38.pyc
│   │   └── view_transformer.cpython-38.pyc
│   ├── apps
│   │   ├── __init__.py
│   │   ├── __pycache__
│   │   │   ├── __init__.cpython-311.pyc
│   │   │   ├── __init__.cpython-38.pyc
│   │   │   ├── cli.cpython-311.pyc
│   │   │   └── cli.cpython-38.pyc
│   │   └── cli.py
│   ├── benchmarker.py
│   ├── detector.py
│   ├── final3.py
│   ├── keypoints_definer.py
│   ├── scripts
│   │   ├── __init__.py
│   │   ├── frame_extractor.py
│   │   ├── middle_finder copy.py
│   │   └── middle_finder.py
│   ├── similarity_calculator.py
│   └── utilities
│       ├── __init__.py
│       ├── __pycache__
│       │   ├── __init__.cpython-310.pyc
│       │   ├── __init__.cpython-311.pyc
│       │   ├── __init__.cpython-38.pyc
│       │   ├── __init__.cpython-39.pyc
│       │   ├── config.cpython-310.pyc
│       │   ├── config.cpython-311.pyc
│       │   ├── config.cpython-38.pyc
│       │   ├── config.cpython-39.pyc
│       │   ├── track_processor.cpython-38.pyc
│       │   ├── utils.cpython-310.pyc
│       │   ├── utils.cpython-38.pyc
│       │   ├── utils.cpython-39.pyc
│       │   └── view_transformer.cpython-38.pyc
│       ├── config.py
│       ├── track_processor.py
│       ├── utils.py
│       └── view_transformer.py
├── keypoints
│   └── new_track
│       ├── all_path.npy
│       ├── all_path_offset.npy
│       ├── all_track.npy
│       ├── center_path.npy
│       ├── keypoints_from_camera.npy
│       ├── keypoints_from_diagram.npy
│       └── offset_path.npy
├── notebooks
│   └── calculate_transformation_error.ipynb
├── poetry.lock
├── pyproject.toml
├── requirements.txt
├── robot_path.npy
├── setup.py
├── tests
│   └── __init__.py
├── tmp.py
└── weights
    └── best.pt

15 directories, 83 files
```

I will provide you with the main modules contents after this, understand this for now!


here is the `drivenetbench/apps/cli.py`:
```
"""Command Line Interface for DriveNetBench."""

from typing import Optional

from typer import Typer

import drivenetbench.utilities.config as configs
from drivenetbench.benchmarker import BenchMarker
from drivenetbench.keypoints_definer import KeyPointsDefiner
from drivenetbench.utilities.track_processor import TrackProcessor

app = Typer()


@app.command()
def define_keypoints(config_file_path: Optional[str] = "config.yaml"):
    """Define Keypoints."""
    configs.load_config(config_file_path)
    keypoints_definer = KeyPointsDefiner()
    keypoints_definer.run()


@app.command()
def extract_track(config_file_path: Optional[str] = "config.yaml"):
    """Extract the track."""
    configs.load_config(config_file_path)
    track_processor = TrackProcessor()
    track_processor.process()


@app.command()
def benchmark(config_file_path: Optional[str] = "config.yaml"):
    """Benchmark the DriveNetBench."""
    bench_marker = BenchMarker(config_file_path)
    bench_marker.run()


if __name__ == "__main__":
    app()
```

here is the `drivenetbench/utilities/track_processor.py`:
```
import logging
from ast import literal_eval
from typing import Optional

import cv2
import numpy as np
from sklearn.cluster import DBSCAN

import drivenetbench.utilities.config as configs

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


class TrackProcessor:
    """A class to process a track image in order to extract either:
    - All center lines (via skeletonization), OR
    - The single offset center path of the outer boundary (via erosion),
    THEN cluster the resulting points to remove noise.
    """

    def __init__(
        self,
        image_path: Optional[str] = None,
        color_hsv: Optional[tuple] = None,
        output_image_path: Optional[str] = None,
        output_npy_path: Optional[str] = None,
        only_offset_outer: Optional[bool] = None,
    ):
        """
        Parameters
        ----------
        image_path : str
            Path to the input track image.
        color_hsv : tuple
            The HSV color of the track in the color-picker scale:
            (H in [0..360], S in [0..100], V in [0..100]).
        output_image_path : str
            Path to save the annotated result (PNG or JPG).
        output_npy_path : str
            Path to save the center path as a .npy 2D array.
        only_offset_outer : bool
            If False, skeletonize the entire track to get all center lines.
            If True, offset just the largest outer boundary to find its center path.
        """
        self.image_path = (
            image_path
            if image_path is not None
            else configs.get_config("track_processor.image_path")
        )
        self.color_hsv_picker = (
            color_hsv
            if color_hsv is not None
            else configs.get_config("track_processor.color_hsv")
        )  # e.g. (330, 23, 84)
        self.color_hsv_picker = literal_eval(self.color_hsv_picker)
        self.output_image_path = (
            output_image_path
            if output_image_path is not None
            else configs.get_config("track_processor.output_image_path_export")
        )
        self.output_npy_path = (
            output_npy_path
            if output_npy_path is not None
            else configs.get_config("track_processor.output_npy_path_export")
        )
        self.only_offset_outer = (
            only_offset_outer
            if only_offset_outer is not None
            else configs.get_config("track_processor.only_offset_the_outer")
        )

    @staticmethod
    def _skeletonize_binary_mask(binary_mask: np.ndarray) -> np.ndarray:
        """Morphologically thins a binary mask (255=foreground, 0=background) to obtain a 1-pixel-wide skeleton.

        Parameters
        ----------
        binary_mask : np.ndarray
            A binary mask of the image to be skeletonized.

        Returns
        -------
        np.ndarray
            The skeletonized image.
        """
        # Ensure strictly 0 or 255
        binary_mask = np.where(binary_mask > 0, 255, 0).astype(np.uint8)

        skeleton = np.zeros_like(binary_mask, dtype=np.uint8)
        temp = binary_mask.copy()
        kernel = cv2.getStructuringElement(cv2.MORPH_CROSS, (3, 3))

        while True:
            eroded = cv2.erode(temp, kernel)
            opened = cv2.dilate(eroded, kernel)
            # Pixels that disappear after an opening => part of the skeleton
            temp2 = cv2.subtract(temp, opened)
            skeleton = cv2.bitwise_or(skeleton, temp2)
            temp = eroded.copy()

            if cv2.countNonZero(temp) == 0:
                break

        return skeleton

    @staticmethod
    def _estimate_track_width(mask: np.ndarray) -> float:
        """Estimates the average track width in pixels for the given track mask.

        Parameters
        ----------
        mask : np.ndarray
            A binary mask of the track (255=track, 0=background).

        Returns
        -------
        float
            The estimated track width in pixels.
        """
        dist = cv2.distanceTransform(mask, cv2.DIST_L2, 5)
        skeleton = TrackProcessor._skeletonize_binary_mask(mask)

        skel_pixels = np.where(skeleton > 0)
        dist_vals = dist[skel_pixels]

        if len(dist_vals) == 0:
            # No skeleton => track not found or too thin
            return 0.0

        median_dist = np.median(dist_vals)
        track_width_est = 2.0 * median_dist
        return track_width_est

    @staticmethod
    def _convert_picker_hsv_to_opencv(hsv_picker: tuple) -> tuple:
        """
        Converts HSV from color-picker scale (H in [0..360], S in [0..100], V in [0..100]) into OpenCV's HSV scale (H in [0..179], S in [0..255], V in [0..255]).

        Parameters
        ----------
        hsv_picker : tuple
            The HSV color in the color-picker scale.

        Returns
        -------
        tuple
            The HSV color in OpenCV's scale.
        """
        h_picker, s_picker, v_picker = hsv_picker

        # Hue:   [0..360] -> [0..179]
        h_cv = int(np.clip(h_picker * 179.0 / 360.0, 0, 179))
        # Saturation: [0..100] -> [0..255]
        s_cv = int(np.clip(s_picker * 2.55, 0, 255))
        # Value: [0..100] -> [0..255]
        v_cv = int(np.clip(v_picker * 2.55, 0, 255))

        return (h_cv, s_cv, v_cv)

    @staticmethod
    def _cluster_and_filter(
        points: np.ndarray,
        eps: float = 5.0,
        min_samples: int = 10,
        cluster_size_threshold: int = 30,
    ) -> np.ndarray:
        """Cluster 2D points (x,y) with DBSCAN, and filter out small clusters or noise.

        Parameters
        ----------
        points : np.ndarray
            The 2D points to be clustered.
        eps : float
            The maximum distance between two samples for one to be considered as in the neighborhood of the other.
        min_samples : int
            The number of samples (or total weight) in a neighborhood for a point to be considered as a core point.
        cluster_size_threshold : int
            The minimum number of points required to keep a cluster.

        Returns
        -------
        np.ndarray
            The filtered points.
        """
        if len(points) == 0:
            return points

        db = DBSCAN(eps=eps, min_samples=min_samples)
        labels = db.fit_predict(points)

        unique_labels, counts = np.unique(labels, return_counts=True)
        filtered_points = []

        for lbl, count in zip(unique_labels, counts):
            if lbl == -1:
                continue  # -1 is "noise" in DBSCAN
            if count >= cluster_size_threshold:
                filtered_points.append(points[labels == lbl])

        if not filtered_points:
            # If everything is small or noise, fallback to returning original
            return points

        return np.vstack(filtered_points)

    def process(self) -> tuple:
        """
        Orchestrates the track detection, either skeletonizing for all center lines or offsetting for the outer boundary center path. Then clusters the resulting points to remove noise. Saves results to disk.

        Returns
        -------
        tuple
            A tuple containing the outer contour points and the center path points.
        """
        image = cv2.imread(self.image_path)
        if image is None:
            raise ValueError(
                f"Could not read image from path: {self.image_path}"
            )

        (h_cv, s_cv, v_cv) = self._convert_picker_hsv_to_opencv(
            self.color_hsv_picker
        )

        h_margin = 15
        s_min = max(s_cv - 40, 0)
        v_min = max(v_cv - 40, 0)
        lower_bound = np.array([max(h_cv - h_margin, 0), s_min, v_min])
        upper_bound = np.array([min(h_cv + h_margin, 179), 255, 255])

        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
        mask = cv2.inRange(hsv, lower_bound, upper_bound)

        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)

        contours, _ = cv2.findContours(
            mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE
        )
        if not contours:
            raise ValueError(
                "No contours found for the specified track color!"
            )
        largest_contour = max(contours, key=cv2.contourArea)
        outer_contour_xy = largest_contour.reshape(-1, 2)  # shape (N,2)

        if not self.only_offset_outer:
            skeleton = self._skeletonize_binary_mask(mask)
            center_pixels = np.column_stack(
                np.where(skeleton > 0)
            )  # (row, col)
            center_line_xy = center_pixels[:, ::-1]  # (x, y)
        else:
            track_width_est = self._estimate_track_width(mask)
            if track_width_est < 1.0:
                logger.warning("Estimated track width < 1. Defaulting to 1.")
                track_width_est = 1.0

            half_width = int(round(track_width_est / 2))
            outer_mask = np.zeros_like(mask, dtype=np.uint8)
            cv2.drawContours(outer_mask, [largest_contour], -1, 255, -1)

            erode_ksize = 2 * half_width + 1
            erode_kernel = cv2.getStructuringElement(
                cv2.MORPH_ELLIPSE, (erode_ksize, erode_ksize)
            )
            offset_mask = cv2.erode(outer_mask, erode_kernel, iterations=1)

            offset_contours, _ = cv2.findContours(
                offset_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE
            )
            if not offset_contours:
                raise ValueError(
                    "Offset contour not found. Track may be too narrow for that offset."
                )
            offset_contour = max(offset_contours, key=cv2.contourArea)
            center_line_xy = offset_contour.reshape(-1, 2)

        eps = configs.get_config("track_processor.dbscan.eps")
        min_samples = configs.get_config("track_processor.dbscan.min_samples")
        cluster_size_threshold = configs.get_config(
            "track_processor.dbscan.cluster_size_threshold"
        )
        center_line_xy = self._cluster_and_filter(
            center_line_xy,
            eps=eps,
            min_samples=min_samples,
            cluster_size_threshold=cluster_size_threshold,
        )

        cv2.drawContours(image, [largest_contour], -1, (0, 255, 0), 2)
        for x, y in center_line_xy:
            cv2.circle(image, (x, y), 1, (0, 0, 255), -1)

        cv2.imwrite(self.output_image_path, image)
        print(f"Annotated image saved to: {self.output_image_path}")

        np.save(self.output_npy_path, center_line_xy)
        print(
            f"Center path (shape {center_line_xy.shape}) saved to: {self.output_npy_path}"
        )

        return outer_contour_xy, center_line_xy


if __name__ == "__main__":
    # Example color: H=330, S=23, V=84 from color picker scale
    # We want all skeleton lines, not just the offset.
    # this tool was used: https://pinetools.com/image-color-picker to get the HSV values

    # processor = TrackProcessor(
    #     image_path="assets/new_track/track-v2.jpg",
    #     color_hsv=(330, 23, 84),  # (H,S,V) in [0..360,0..100,0..100]
    #     output_image_path="assets/annotated.png",
    #     output_npy_path="keypoints/new_track/all_path.npy",
    #     only_offset_outer=False,
    # )
    # outer_points, center_points = processor.process()
    # print(f"Outer contour has {len(outer_points)} points.")
    # print(f"Center path has {len(center_points)} points.")

    processor = TrackProcessor()
    processor.process()
```

here is the `drivenetbench/utilities/view_transformer.py`:
```
"""View transformer module."""

from typing import List, Optional, Tuple

import cv2
import numpy as np
import numpy.typing as npt


class ViewTransformer:
    def __init__(
        self, source: npt.NDArray[np.float32], target: npt.NDArray[np.float32]
    ) -> None:
        """Initialize the ViewTransformer.

        Parameters
        ----------
        source : npt.NDArray[np.float32]
            The source points.
        target : npt.NDArray[np.float32]
            The target points.
        """
        if source.xy.shape != target.xy.shape:
            raise ValueError("Source and target must have the same shape.")

        source_pts = source.xy.copy()
        target_pts = target.xy.copy()
        source_pts = source_pts.squeeze(axis=0)
        target_pts = target_pts.squeeze(axis=0)

        if source_pts.shape[1] != 2:
            raise ValueError(
                "Source and target points must be 2D coordinates."
            )

        self.source_pts = source_pts.astype(np.float32)
        self.target_pts = target_pts.astype(np.float32)
        self.m, _ = cv2.findHomography(self.source_pts, self.target_pts)

        if self.m is None:
            raise ValueError("Homography matrix must could not be calculated.")
        self.avg_error = None
        self.accumulated_error = None

    def transform_points(self, points: npt.NDArray[np.float32]) -> npt.NDArray:
        """Transform the points.

        Parameters
        ----------
        points : npt.NDArray[np.float32]
            The points to transform.

        Returns
        -------
        npt.NDArray
            The transformed points.
        """
        if points.size == 0:
            return points
        if points.shape[1] != 2:
            raise ValueError("Points must be 2D coordinates.")

        reshaped_points = points.reshape(-1, 1, 2).astype(np.float32)
        transformed_points = cv2.perspectiveTransform(reshaped_points, self.m)
        return transformed_points.reshape(-1, 2).astype(np.float32)

    def transform_image(
        self, image: npt.NDArray[np.uint8], resolution_wh: Tuple[int, int]
    ) -> npt.NDArray[np.uint8]:
        """Transform the image.

        Parameters
        ----------
        image : npt.NDArray[np.uint8]
            The image to transform.
        resolution_wh : Tuple[int, int]
            The resolution of the image.

        Returns
        -------
        npt.NDArray
            The transformed image.
        """
        if len(image.shape) not in {2, 3}:
            raise ValueError("Image must be either grayscale or color.")

        return cv2.warpPerspective(image, self.m, resolution_wh)

    def calculate_transformation_error(
        self,
        source: Optional[npt.NDArray] = None,
        target: Optional[npt.NDArray] = None,
        is_source_transformed: bool = False,
    ) -> Tuple[float, float]:
        """Calculate the transformation error.

        Parameters
        ----------
        source : npt.NDArray, optional
            The source points (default is None), if None, the source points used in the initialization are used.
        target : npt.NDArray, optional
            The target points (default is None), if None, the target points used in the initialization are used
        is_source_transformed : bool, optional
            Whether the source points are transformed (default is False).

        Returns
        -------
        Tuple[float, float]
            The average error and the accumulated error.
        """
        source = source if isinstance(source, np.ndarray) else self.source_pts
        target = target if isinstance(target, np.ndarray) else self.target_pts

        source = (
            source if is_source_transformed else self.transform_points(source)
        )

        self.accumulated_error = 0
        for source_pt, target_pt in zip(source, target):
            error = np.linalg.norm(source_pt - target_pt)
            self.accumulated_error += error

        self.avg_error = self.accumulated_error / len(source)
        return self.avg_error, self.accumulated_error


if __name__ == "__main__":
    import numpy as np
    import supervision as sv

    original_source_keypoints = np.load(
        "/Users/aalbustami/UMD/BIMI/projects/broverette/DriveNetBench/keypoints/keypoints_from_camera.npy"
    )
    target_keypoints = np.load(
        "/Users/aalbustami/UMD/BIMI/projects/broverette/DriveNetBench/keypoints/keypoints_from_digital.npy"
    )
    source_keypoints = original_source_keypoints[np.newaxis, ...]
    target_keypoints = target_keypoints[np.newaxis, ...]

    source_keypoints = sv.KeyPoints(source_keypoints)
    target_keypoints = sv.KeyPoints(target_keypoints)
    transformer = ViewTransformer(
        source=source_keypoints, target=target_keypoints
    )

    print(transformer.calculate_transformation_error())
```

understand these for now I give you more modules later!


here is the `drivenetbench/benchmarker.py`:
```
"""Benchmarker class for running benchmarks on DriveNet models."""

import os

import cv2
import numpy as np
import numpy.typing as npt
import toml

import drivenetbench.utilities.config as configs
from drivenetbench.detector import Detector
from drivenetbench.similarity_calculator import SimilarityCalculator
from drivenetbench.utilities.utils import load_and_preprocess_points
from drivenetbench.utilities.view_transformer import ViewTransformer


class BenchMarker:
    """Class for running benchmarks on DriveNet models."""

    def __init__(self, config_file_path: str):
        """Initialize the BenchMarker.

        Parameters
        ----------
        config_file_path : str
            The path to the configuration file.
        """
        self.config_file_path = config_file_path

        os.mkdir("results") if not os.path.exists("results") else None

        configs.load_config(config_file_path)

        self.detector = Detector()
        self.similarity_calculator = SimilarityCalculator()

        self.view_transformer = None
        self._load_view_transformer()

        self.reference_path = None
        self.track_image = None
        self._annotate_path_to_track()

        video_path = configs.get_config("benchmarker.video_path")
        self.video_name = self.experiment_name = os.path.basename(
            video_path
        ).split(".")[0]

        self._create_results_directory()

        self.cap = cv2.VideoCapture(video_path)
        self.detector.frame_height = self.frame_height = int(
            self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT)
        )
        self.frame_width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        self.video_fps = int(self.cap.get(cv2.CAP_PROP_FPS))

    def _load_view_transformer(self):
        """Load the view transformer from the configuration file."""
        source_keypoints_path = configs.get_config(
            "view_transformer.source_path"
        )
        target_keypoints_path = configs.get_config(
            "view_transformer.target_path"
        )

        source_keypoints = load_and_preprocess_points(source_keypoints_path)
        target_keypoints = load_and_preprocess_points(target_keypoints_path)

        self.view_transformer = ViewTransformer(
            source=source_keypoints, target=target_keypoints
        )

    def _annotate_path_to_track(self):
        """Annotate the reference path to the track image."""
        reference_path_file_path = configs.get_config(
            "benchmarker.reference_track_npy_path"
        )
        self.reference_path = np.load(reference_path_file_path)

        track_image_path = configs.get_config("benchmarker.track_image_path")
        self.track_image = cv2.imread(track_image_path)
        for point in self.reference_path:
            cv2.circle(self.track_image, tuple(point), 5, (76, 39, 0), -1)

    def _create_results_directory(self):
        """Create the results directory."""
        self.results_dir = os.path.join("results", self.experiment_name)
        if not os.path.exists(self.results_dir):
            os.makedirs(self.results_dir, exist_ok=True)
            return

        RESULTS_COMPONENETS = [
            "results.toml",
            "robot_path.npy",
            "final.jpg",
        ]
        self.save_video = configs.get_config("actions.save_live_to_disk")
        if self.save_video:
            RESULTS_COMPONENETS.append("final.mp4")
        all_present = all(
            os.path.exists(os.path.join(self.results_dir, comp))
            for comp in RESULTS_COMPONENETS
        )
        if all_present:
            raise FileExistsError(
                f"Results directory {self.results_dir} already exists."
            )

        os.makedirs(self.results_dir, exist_ok=True)

    def run(self):
        """Run the benchmarker."""
        robot_path = (
            self._detect_robot_path()
            if not os.path.exists(
                os.path.join(self.results_dir, "robot_path.npy")
            )
            else np.load(os.path.join(self.results_dir, "robot_path.npy"))
        )
        robot_path = robot_path[~np.all(robot_path == -1, axis=1)]
        distance_threshold = configs.get_config(
            "benchmarker.time.distance_threshold_in_pixels"
        )
        skip_seconds = configs.get_config(
            "benchmarker.time.skip_first_x_seconds"
        )

        time_to_return = self._calculate_time_for_full_rotation(
            robot_path,
            fps=self.video_fps,
            distance_threshold=distance_threshold,
            skip_seconds=skip_seconds,
        )

        similarity_percentage = (
            self.similarity_calculator.calculate_path_similarity(
                robot_path, self.reference_path
            )
        )

        results = {
            "time_to_return": time_to_return,
            "similarity_percentage": similarity_percentage,
        }
        with open(os.path.join(self.results_dir, "results.toml"), "w") as f:
            toml.dump(results, f)

    def _detect_robot_path(self) -> npt.NDArray:
        """Detect the robot path in the video.

        Returns
        -------
        npt.NDArray
            The robot path.

        Raises
        ------
        IOError
            If the video stream or file cannot be opened.
        """
        if not self.cap.isOpened():
            raise IOError("Error opening video stream or file")

        self.show_live = configs.get_config("actions.show_live")

        video_writer = (
            cv2.VideoWriter(
                os.path.join(self.results_dir, "final.mp4"),
                cv2.VideoWriter_fourcc(*"mp4v"),
                self.video_fps,
                (self.frame_width, self.frame_height),
            )
            if self.save_video
            else None
        )
        robot_path = []
        while self.cap.isOpened():
            ret, frame = self.cap.read()
            if not ret:
                break

            current_frame = self.cap.get(cv2.CAP_PROP_POS_FRAMES)
            max_seconds = configs.get_config(
                "actions.early_stop_after_x_seconds"
            )

            max_frames = None
            if max_seconds:
                max_frames = max_seconds * self.video_fps
            if max_frames is not None and current_frame > max_frames:
                break

            robot_centroid: npt.NDArray = self.detector.detect_robot(
                frame=frame
            )
            if robot_centroid is not None:
                robot_path.append(robot_centroid)
            else:
                robot_path.append(np.array([-1, -1]))
                continue

            transformed_centroid = self._get_transformed_point(robot_centroid)

            if self.show_live:
                cv2.circle(
                    self.track_image,
                    tuple(transformed_centroid.astype(int)),
                    20,
                    (5, 203, 255),
                    -1,
                )
                cv2.polylines(
                    frame,
                    [self.detector.points],
                    isClosed=True,
                    color=(0, 255, 0),
                    thickness=2,
                )
                cv2.circle(
                    frame,
                    tuple(robot_centroid.astype(int)),
                    5,
                    (0, 255, 0),
                    -1,
                )

                cv2.namedWindow("Robot Detection", cv2.WINDOW_NORMAL)
                cv2.imshow("Robot Detection", frame)

                cv2.namedWindow("Track View", cv2.WINDOW_NORMAL)
                cv2.imshow("Track View", self.track_image)

                if cv2.waitKey(1) & 0xFF == ord("q"):
                    print("User requested exit. Stopping early...")
                    break
            if video_writer is not None:
                video_writer.write(frame)
        cv2.imwrite(
            os.path.join(self.results_dir, "final.jpg"), self.track_image
        )
        video_writer.release() if self.save_video else None
        self.cap.release()
        cv2.destroyAllWindows()

        robot_path = np.array(robot_path, dtype=np.float32)
        np.save(os.path.join(self.results_dir, "robot_path.npy"), robot_path)

        return robot_path

    def _get_transformed_point(self, point: npt.NDArray) -> npt.NDArray:
        """Get the transformed point.

        Parameters
        ----------
        point : npt.NDArray
            The point to transform.

        Returns
        -------
        npt.NDArray
            The transformed point.
        """
        centroid_2d = point[np.newaxis, ...]
        transformed_centroid = self.view_transformer.transform_points(
            centroid_2d
        )
        squeezed_transformed_centroid = np.squeeze(
            transformed_centroid, axis=0
        )
        return squeezed_transformed_centroid

    def _calculate_time_for_full_rotation(
        self,
        robot_path: npt.NDArray,
        fps: int,
        distance_threshold: float = 50.0,
        skip_seconds: int = 5,
    ) -> float:
        """
        Calculates how long it takes for the robot to make a "full rotation" and return close
        to the first point in its path.

        Parameters
        ----------
        robot_path : npt.NDArray, shape (N, 2)
            The robot's path in track coordinates, in chronological order.
        fps : int
            Frames per second of the source video, used to convert frames to seconds.
        distance_threshold : float, optional
            If the robot comes within this Euclidean distance of the start point, we
            consider it has returned.
        start_frame : int, optional

        Returns
        -------
        float
            Time in seconds it took to complete the rotation. If it never returns, returns -1.
        """
        if len(robot_path) < 2:
            raise ValueError("Robot path must have at least two points.")

        start_frame = int(skip_seconds * fps)
        start_point = robot_path[0]
        for i in range(start_frame, len(robot_path)):
            dist = np.linalg.norm(robot_path[i] - start_point)
            if dist <= distance_threshold:
                # i => the frame index of the path, time => i / fps, assuming each frame has a robot position
                return i / fps

        return -1.0  # never returned to start


if __name__ == "__main__":
    benchmarker = BenchMarker("config.yaml")
    benchmarker.run()
```

here is the `drivenetbench/detector.py`:
```
from typing import Union

import cv2
import numpy as np
import numpy.typing as npt
from ultralytics import YOLO
from ultralytics.engine.results import Results

import drivenetbench.utilities.config as configs


class Detector:
    """Class for detecting the robot in the video."""

    def __init__(self):
        """Initialize the Detector."""
        self.frame_height = None
        model_path = configs.get_config(
            "benchmarker.detection_model.model_path"
        )
        self.model = YOLO(model_path)

        self.confidence_threshold = configs.get_config(
            "benchmarker.detection_model.conf_threshold"
        )
        self.shift_ratio = configs.get_config(
            "benchmarker.detection_model.shift_ratio"
        )

    def detect_robot(self, frame: npt.NDArray) -> Union[npt.NDArray, None]:
        """Detect the robot in the frame.

        Parameters
        ----------
        frame : npt.NDArray
            The frame to detect the robot in.

        Returns
        -------
        Union[npt.NDArray, None]
            The centroid of the robot.
        """
        results, *_ = self.model(frame)
        centriod = self._post_process(results=results)

        return centriod

    def _post_process(self, results: Results) -> Union[npt.NDArray, None]:
        """Post process the results.

        Parameters
        ----------
        results : Results
            The results to post process.

        Returns
        -------
        Union[npt.NDArray, None]
            The shifted centroid of the robot.
        """
        if results.obb is None:
            return None

        shifted_centroid = None
        for box in results.obb:
            if box.conf[0] < self.confidence_threshold:
                continue

            corners = box.xyxyxyxy.cpu().numpy().squeeze()
            self.points = np.array(
                [
                    [int(corners[0][0]), int(corners[0][1])],
                    [int(corners[1][0]), int(corners[1][1])],
                    [int(corners[2][0]), int(corners[2][1])],
                    [int(corners[3][0]), int(corners[3][1])],
                ],
                dtype=np.int32,
            )

            current_shifted_centroid = self._extract_centroid(self.points)
            if current_shifted_centroid is not None:
                shifted_centroid = current_shifted_centroid
                break
            if shifted_centroid is None:
                return None

        return shifted_centroid

    def _extract_centroid(self, points: npt.NDArray) -> npt.NDArray:
        """Extract the centroid of the robot.

        Parameters
        ----------
        points : npt.NDArray
            The points to extract the centroid from.

        Returns
        -------
        npt.NDArray
            The centroid of the robot.
        """
        centroid = np.mean(points, axis=0)

        if centroid[0] > 2700 and centroid[1] < 100:
            return None

        y_diff = self.frame_height - centroid[1]
        shift = int(y_diff * self.shift_ratio)

        shited_centroid = np.array([centroid[0], centroid[1] + shift])

        return shited_centroid
```

Understand these I will add more


here is the `drivenetbench/keypoints_definer.py`:
```
"""Utility functions for Easier POC applications."""

import tkinter
from tkinter import Canvas, Tk
from typing import Optional

import cv2
import numpy as np
import numpy.typing as npt
from PIL import Image, ImageTk

import drivenetbench.utilities.config as configs
from drivenetbench.utilities.utils import path_checker


class KeyPointsDefiner:
    """Class for defining keypoints on a video or image source."""

    def __init__(
        self,
        source: Optional[str] = None,
        output_name: Optional[str] = None,
        frame_num: Optional[int] = None,
        override_if_exists: Optional[bool] = False,
    ):
        """Initialize the KeyPointsDefiner.

        Parameters
        ----------
        source : str
            The source of the video or image.
        output_name : str
            The name of the output numpy file.
        frame_num : int, optional
            The frame number to extract for videos (default is 1).
        override_if_exists : bool, optional
            Whether to override the output file if it exists (default is False).
        """
        self.source = (
            source
            if source is not None
            else configs.get_config("keypoints_definer.source_path")
        )
        self.output_name = (
            output_name
            if output_name is not None
            else configs.get_config("keypoints_definer.output_name")
        )
        self.frame_num = (
            frame_num
            if frame_num is not None
            else configs.get_config("keypoints_definer.frame_number")
        )
        self.override_if_exists = (
            override_if_exists
            if override_if_exists
            else configs.get_config("keypoints_definer.override_if_exists")
        )

        if path_checker(f"{self.output_name}.npy", break_if_not_found=False)[
            0
        ]:
            if not override_if_exists:
                raise FileExistsError(
                    f"Output file {self.output_name}.npy already exists. "
                    "Use the --override-if-exists flag to override."
                )

        self.frame_num = frame_num

        self.window = None
        self.canvas = None
        self.points = []
        self.resize_factor = 1
        self.original_frame_width = None
        self.original_frame_height = None
        self.tk_image = None

    def run(self):
        """Run the keyponits definer application."""
        self.setup_window()
        frame = self.extract_frame()
        self.setup_canvas(frame)
        self.setup_bindings()
        self.window.mainloop()

    def setup_window(self):
        """Set up the Tkinter window."""
        self.window = Tk()
        self.window.title("Define KeyPoints")

    def extract_frame(self) -> npt.NDArray:
        """Extract a frame from the source.

        Returns
        -------
        npt.NDArray
            The extracted frame in RGB format.

        Raises
        ------
        Exception
            If the frame cannot be read.
        """
        cap = cv2.VideoCapture(self.source)
        if not cap.isOpened():
            raise Exception(f"Failed to open source: {self.source}")

        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        if total_frames == 1:
            self.frame_num = 0
        else:
            self.frame_num = min(max(0, self.frame_num), total_frames - 1)

        cap.set(cv2.CAP_PROP_POS_FRAMES, self.frame_num)
        ret, frame = cap.read()
        if not ret:
            cap.release()
            raise Exception(f"Failed to read frame {self.frame_num}.")

        self.original_frame_height, self.original_frame_width = frame.shape[:2]
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        cap.release()
        return frame

    def setup_canvas(self, frame: npt.NDArray):
        """Set up the Tkinter canvas with the frame.

        Parameters
        ----------
        frame : npt.NDArray
            The frame to display.
        """
        screen_pad = 200
        screen_width = self.window.winfo_screenwidth() - screen_pad
        screen_height = self.window.winfo_screenheight() - screen_pad

        self.resize_factor = min(
            screen_width / self.original_frame_width,
            screen_height / self.original_frame_height,
        )

        canvas_pad = 100
        window_width = (
            int(self.original_frame_width * self.resize_factor) + canvas_pad
        )
        window_height = (
            int(self.original_frame_height * self.resize_factor) + canvas_pad
        )

        frame = cv2.resize(
            frame, (0, 0), fx=self.resize_factor, fy=self.resize_factor
        )
        self.window.geometry(f"{window_width}x{window_height}")

        resized_height, resized_width = frame.shape[:2]
        image = Image.fromarray(frame)
        self.tk_image = ImageTk.PhotoImage(image)

        self.canvas = Canvas(
            self.window, width=resized_width, height=resized_height
        )
        self.canvas.create_image(0, 0, anchor="nw", image=self.tk_image)
        self.canvas.pack()

    def setup_bindings(self):
        """Set up event bindings for the canvas."""
        self.canvas.bind("<Button-1>", self.on_click)
        self.canvas.bind("<Key>", self.on_key_press)
        self.canvas.focus_set()

    def on_click(self, event: tkinter.Event):
        """Handle mouse click events.

        Parameters
        ----------
        event : tkinter.Event
            The event object.
        """
        x, y = event.x, event.y
        self.points.append((x, y))
        self.update_canvas()

    def on_key_press(self, event: tkinter.Event):
        """Handle key press events.

        Parameters
        ----------
        event : tkinter.Event
            The event object.
        """
        if event.char.lower() == "q":
            if self.points:
                self.save_points()
            self.window.destroy()
        elif event.char.lower() == "z":
            if self.points:
                self.points.pop()
                self.update_canvas()

    def save_points(self):
        """Save the collected points to a numpy file."""
        points_arr = np.array(self.points)
        scale_factor = 1 / self.resize_factor
        points_arr = points_arr * scale_factor
        points_arr = points_arr.astype(int)

        points_arr[:, 0] = np.clip(
            points_arr[:, 0], 0, self.original_frame_width - 1
        )
        points_arr[:, 1] = np.clip(
            points_arr[:, 1], 0, self.original_frame_height - 1
        )

        np.save(f"{self.output_name}.npy", points_arr)

    def update_canvas(self):
        """Update the canvas by redrawing the points and shapes."""
        self.canvas.delete("points")

        for i, point in enumerate(self.points):
            x, y = point
            radius = 3
            self.canvas.create_oval(
                x - radius,
                y - radius,
                x + radius,
                y + radius,
                fill="red",
                outline="red",
                tag="points",
            )
            # write the point index
            self.canvas.create_text(
                x + 10,
                y + 10,
                text=str(i + 1),
                fill="red",
                tag="points",
            )
```

and here is the `drivenetbench/similarity_calculator.py`:
```
from enum import Enum

import numpy as np
import numpy.typing as npt

import drivenetbench.utilities.config as configs


class Methods(Enum):
    """Enum for the methods of path similarity."""

    DTW = 1
    FRECHET = 2


class SimilarityCalculator:
    """Class for calculating the similarity between two paths."""

    def __init__(self):
        """Initialize the SimilarityCalculator."""
        method = configs.get_config("benchmarker.path_similarity.method")
        self.method = Methods[method.upper()]

        self.auto_tune = configs.get_config(
            "benchmarker.path_similarity.auto_tune"
        )
        if self.auto_tune:
            self.clamp_percentage = configs.get_config(
                "benchmarker.path_similarity.clamp_percentage"
            )
            self.clamp_distance = None
            self.distance_baseline = None
        else:
            self.clamp_distance = configs.get_config(
                "benchmarker.path_similarity.clamp_distance"
            )
            self.distance_baseline = configs.get_config(
                "benchmarker.path_similarity.distance_baseline"
            )

    def calculate_path_similarity(
        self,
        robot_path: npt.NDArray,
        reference_path: npt.NDArray,
    ) -> float:
        """Calculate the similarity between two paths.

        Parameters
        ----------
        robot_path : npt.NDArray
            The robot path.
        reference_path : npt.NDArray
            The reference path.

        Returns
        -------
        float
            The similarity percentage.
        """
        self.robot_path = robot_path
        self.reference_path = reference_path

        if self.auto_tune:
            # sets the clamp_distance and distance_baseline
            self._auto_tune_parameters()

        functions_map = {
            Methods.DTW: self._dtw_distance,
            Methods.FRECHET: self._frechet_distance,
        }

        distance = functions_map[self.method](
            self.robot_path, self.reference_path, self.clamp_distance
        )

        raw_ratio = 1.0 - (distance / self.distance_baseline)
        similarity_percent = 100.0 * max(0.0, raw_ratio)

        similarity_percent = min(similarity_percent, 100.0)

        return similarity_percent

    def _auto_tune_parameters(self):
        """Automatically tune the parameters for the similarity calculation."""
        if self.robot_path.size == 0 or self.reference_path.size == 0:
            return 1000.0, 100.0

        combined = np.vstack([self.robot_path, self.reference_path])
        combined_min = combined.min(axis=0)
        combined_max = combined.max(axis=0)

        bounding_diagonal = np.linalg.norm(combined_max - combined_min)

        if bounding_diagonal < 1.0:
            # If the bounding box is extremely tiny, fallback:
            return 1000.0, 100.0

        self.distance_baseline = bounding_diagonal
        # And clamp_distance as ~10% of that diagonal, so large outliers don't explode the cost:
        self.clamp_distance = self.clamp_percentage * bounding_diagonal

    @staticmethod
    def _dtw_distance(
        path_a: npt.NDArray, path_b: npt.NDArray, clamp_dist: float = None
    ) -> float:
        """Compute DTW distance between two 2D paths (N vs M). Lower = more similar.

        Parameters
        ----------
        path_a : npt.NDArray
            First path.
        path_b : npt.NDArray
            Second path.
        clamp_dist : float, optional
            If provided, each pairwise distance is clamped to this max.

        Returns
        -------
        float
            Total DTW cost.
        """
        n, m = len(path_a), len(path_b)
        dtw_matrix = np.full((n + 1, m + 1), np.inf, dtype=np.float32)
        dtw_matrix[0, 0] = 0.0

        for i in range(1, n + 1):
            for j in range(1, m + 1):
                cost = np.linalg.norm(path_a[i - 1] - path_b[j - 1])
                if clamp_dist is not None:
                    cost = min(cost, clamp_dist)

                dtw_matrix[i, j] = cost + min(
                    dtw_matrix[i - 1, j],  # deletion
                    dtw_matrix[i, j - 1],  # insertion
                    dtw_matrix[i - 1, j - 1],  # match
                )

        # The raw sum cost:
        raw_sum = float(dtw_matrix[n, m])
        # Normalize it by the path size:
        normalized_dtw = raw_sum / (n + m)

        return normalized_dtw

    @staticmethod
    def _frechet_distance(
        path_a: npt.NDArray, path_b: npt.NDArray, clamp_dist: float = None
    ) -> float:
        """Iterative Frechet distance between two 2D paths. Lower = more similar.

        Parameters
        ----------
        path_a : npt.NDArray
            First path.
        path_b : npt.NDArray
            Second path.
        clamp_dist : float, optional
            If not None, will clamp each pairwise distance to at most this value
            to soften the penalty for large differences.

        Returns
        -------
        float
            Frechet distance.
        """
        n, m = len(path_a), len(path_b)
        # dp[i,j] will hold the Frechet distance up to path_a[:i+1], path_b[:j+1].
        dp = np.full((n, m), -1.0, dtype=np.float32)

        # Helper to compute local cost with clamp
        def local_dist(i, j):
            d = np.linalg.norm(path_a[i] - path_b[j])
            return min(d, clamp_dist)

        # Initialize first cell
        dp[0, 0] = local_dist(0, 0)

        # First row
        for j in range(1, m):
            dp[0, j] = max(dp[0, j - 1], local_dist(0, j))

        # First column
        for i in range(1, n):
            dp[i, 0] = max(dp[i - 1, 0], local_dist(i, 0))

        # Fill the rest
        for i in range(1, n):
            for j in range(1, m):
                cost_ij = local_dist(i, j)
                dp[i, j] = max(
                    min(dp[i - 1, j], dp[i - 1, j - 1], dp[i, j - 1]), cost_ij
                )

        return float(dp[n - 1, m - 1])
```

Understand these I will add more!

the `weights/best.pt` is a YOLOv10 bbox detection model! trained by me!

and here is the `config.yaml`:
```
benchmarker:
  video_path: "assets/new_track/driver_4.MOV"
  detection_model:
    model_path: "weights/best.pt"
    conf_threshold: 0.85
    shift_ratio: 0.02
  track_image_path: "assets/new_track/track-v2.jpg"
  reference_track_npy_path: "keypoints/new_track/all_path.npy"

  path_similarity:
    method: 'dtw' # dtw, frechet

    # one of these blocks (if `auto_tune` is True, the other block is ignored)
    auto_tune: True
    clamp_percentage: 0.05
    # or
    clamp_distance: 300.0
    distance_baseline: 3500.0

  time:
    distance_threshold_in_pixels: 50.0
    skip_first_x_seconds: 5.0


view_transformer:
  source_path: "keypoints/new_track/keypoints_from_camera.npy"
  target_path: "keypoints/new_track/keypoints_from_diagram.npy"


keypoints_definer:
  source_path: "assets/new_track/base.jpg"
  output_name: "test" # without .npy
  frame_number: 1 # if source is video
  override_if_exists: False


track_processor:
  image_path: "assets/new_track/track-v2.jpg"
  color_hsv: (330, 23, 84) # use https://pinetools.com/image-color-picker
  output_image_path_export: "assets/new_track/annotated_track.jpg"
  output_npy_path_export: "keypoints/new_track/all_track.npy"
  only_offset_the_outer: False
  dbscan:
    eps: 5.0
    min_samples: 5
    cluster_size_threshold: 30


actions:
  early_stop_after_x_seconds: # leave empty to disable
  show_live: True
  save_live_to_disk: True
```

You are a professional graduate researcher, you are my assistant to write a research paper that will be submitted to top tier IEEE conference.

I want you to understand all the files, structure, functionality, usage and everything related to the project!
I will ask you after this!


I want you to write me the research paper of this project to be sent to top tier IEEE conference!

The main idea of the project is that I have created a new tool that uses only one camera to benchmark the drive networks (the neural networks and agents that are trained in the industrial research labs and in the universities). my tool is easily replicable, affordable and configurable.