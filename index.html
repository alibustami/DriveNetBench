<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="DriveNetBench: an open source single-camera-based benchmarking system for evaluating autonomous driving networks.">
  <meta name="keywords" content="DriveNetBench, Autonomous Driving, Benchmark, Neural Networks">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DriveNetBench</title>

  <!-- Global site tag (gtag.js) - Google Analytics (Remove or replace with your own if desired) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());
    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- Navigation Bar (kept exactly the same style) -->
<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <!-- <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div> -->
  </div>
</nav>
<!-- / Navigation Bar -->


<!-- Title & Authors -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">DriveNetBench: An Affordable and Configurable Single-Camera Benchmarking System for Autonomous Driving Networks</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://google.com">Ali Al Bustami</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://google.com">Humberto Ruiz-Ochoa</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://google.com">Jaerock Kwon</a><sup>1</sup>
            </span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Michigan</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Paper Link -->
              <span class="link-block">
                <a href="https://google.com"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Arxiv Link -->
              <span class="link-block">
                <a href="https://google.com"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Toolkit (GitHub) Link -->
              <span class="link-block">
                <a href="https://github.com/alibustami/DriveNetBench"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>GitHub</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- / Title & Authors -->

<!-- Main Content (Readme) -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">

        <!-- BEGIN: Copied README content, badges removed -->

        <div class="content has-text-justified">

          <h2 class="title is-3">DriveNetBench</h2>
          <p><em>An Affordable, Single-Camera Benchmarking System for Evaluating Autonomous Driving Networks with a modular approach to track, transform, and test your driving models.</em></p>

          <figure>
            <img src="./assets/bench.gif" alt="DriveNetBench demonstration">
          </figure>

          <hr>

          <h2 class="title is-3">Project Description</h2>
          <p>
            DriveNetBench is a benchmarking tool designed to evaluate the performance of neural networks
            for autonomous driving control. The tool uses a single camera to navigate predefined tracks and
            provides metrics to assess the accuracy and efficiency of the neural networks. The main features
            of DriveNetBench include:
          </p>
          <ul>
            <li>Detection and tracking of the robot using a YOLO model.</li>
            <li>Calculation of path similarity using Dynamic Time Warping (DTW) or Frechet distance.</li>
            <li>Extraction and processing of track images to define keypoints and paths.</li>
            <li>Visualization of the robot's path and track.</li>
          </ul>

          <hr>

          <h2 class="title is-3">System Overview</h2>
          <p>
            Below are three key figures from our DriveNetBench research paper that illustrate
            the overall workflow, the system architecture, and the supporting modules used in
            the benchmarking process.
          </p>

          <h3 class="title is-4">Figure 1: DriveNetBench Workflow</h3>
          <figure>
            <img src="./assets/DriveNetBench Diagram.png" alt="Figure 1 – High-Level Workflow">
            <figcaption>
              <em>
                This figure shows an overview of the single-camera DriveNetBench workflow:
                A robot is recorded by an overhead camera, and DriveNetBench evaluates that footage
                using configurable settings and performance metrics (e.g., path similarity, completion time).
              </em>
            </figcaption>
          </figure>

          <h3 class="title is-4">Figure 2: DriveNetBench System Architecture</h3>
          <figure>
            <img src="./assets/architecture_diagram.png" alt="Figure 2 – System Architecture">
            <figcaption>
              <em>
                Here is the architecture that processes the camera feed, detects the robot, applies
                a perspective transform, and compares the resulting positions against a digital “track.”
                DriveNetBench reports path similarity and rotation time, with optional homography-error
                checks to ensure calibration.
              </em>
            </figcaption>
          </figure>

          <h3 class="title is-4">Figure 3: Supporting Modules</h3>
          <figure>
            <img src="./assets/supporting_modules.png" alt="Figure 3 – Supporting Modules">
            <figcaption>
              <em>
                This figure illustrates how DriveNetBench’s “TrackProcessor,” “KeyPointsDefiner,”
                “ViewTransformer,” and “Detector” all work together. The TrackProcessor extracts
                the centerline route from the digital track; the KeyPointsDefiner and ViewTransformer
                align camera-space coordinates with track-space coordinates; and the Detector identifies
                the robot’s position, enabling the path-comparison metrics.
              </em>
            </figcaption>
          </figure>

          <hr>

          <h2 class="title is-3">Requirements</h2>
          <ul>
            <li>Miniconda or Anaconda</li>
          </ul>

          <hr>

          <h2 class="title is-3">Installation</h2>
          <p>To install DriveNetBench, follow these steps:</p>
          <ol>
            <li>Clone the repository:
              <pre><code>git clone https://github.com/alibustami/DriveNetBench.git
cd DriveNetBench
</code></pre>
            </li>

            <li>
              <strong>Unix</strong>: Create a virtual environment and build the package:
              <pre><code>make create-venv
</code></pre>
            </li>

            <li>
              <strong>Windows</strong>: Create a virtual environment and build the package:
              <pre><code>conda create -n drivenetbench python=3.8
conda activate drivenetbench
python -m pip install --upgrade pip
python -m pip install --upgrade setuptools wheel
python -m pip install -e .
pre-commit install
</code></pre>
            </li>
          </ol>

          <hr>

          <h2 class="title is-3">Configuration</h2>
          <p>
            The <code>config.yaml</code> file is the only file you need to modify in the entire repository.
            This file contains all the necessary configurations for running the benchmark.
          </p>

          <h3 class="title is-4">Configuration Parameters</h3>
          <p>
            The <code>config.yaml</code> file contains the following parameters:
          </p>
          <ul>
            <li>
              <strong>Benchmarker class</strong>:
              <ul>
                <li><code>benchmarker.video_path</code>: The path to the video file to be benchmarked.</li>
                <li><code>benchmarker.detection_model.model_path</code>: The path to the YOLO model weights.</li>
                <li><code>benchmarker.detection_model.conf_threshold</code>: The confidence threshold for the YOLO model.</li>
                <li><code>benchmarker.detection_model.shift_ratio</code>: The ratio to shift the detection centroid.</li>
                <li><code>benchmarker.track_image_path</code>: The path to the track image.</li>
                <li><code>benchmarker.reference_track_npy_path</code>: The path to the reference track numpy file.</li>
                <li><code>benchmarker.path_similarity.method</code>: The method used to calculate the path similarity. Available: <code>dtw</code> or <code>frechet</code>.</li>
                <li><code>benchmarker.path_similarity.auto_tune</code>: Boolean for automatic tuning of <code>clamp_distance</code> and <code>distance_baseline</code>.</li>
                <li><code>benchmarker.path_similarity.clamp_percentage</code>: Percentage of the path length used as the clamp distance.</li>
                <li><code>benchmarker.path_similarity.clamp_distance</code>: The max distance between two points considered a match.</li>
                <li><code>benchmarker.path_similarity.distance_baseline</code>: The distance baseline for matching points.</li>
                <li><code>benchmarker.time.distance_threshold_in_pixels</code>: Linear distance between the robot's start and end points to consider a full rotation.</li>
                <li><code>benchmarker.time.skip_first_x_seconds</code>: Number of seconds to skip at the start of the video.</li>
              </ul>
            </li>
            <li>
              <strong>ViewTransform class</strong>:
              <ul>
                <li><code>view_transformer.source_path</code>: The path to the source image (camera view).</li>
                <li><code>view_transformer.target_path</code>: The path to the target image (digital graph).</li>
              </ul>
            </li>
            <li>
              <strong>KeyPointsDefiner class</strong>:
              <ul>
                <li><code>keypoints_definer.source_path</code>: The image path for defining keypoints.</li>
                <li><code>keypoints_definer.output_name</code>: The name of the output file for the keypoints.</li>
                <li><code>keypoints_definer.frame_number</code>: The frame number to define keypoints (if source is a video).</li>
                <li><code>keypoints_definer.override_if_exists</code>: Whether to overwrite the output file if it exists.</li>
              </ul>
            </li>
            <li>
              <strong>TrackProcessor class</strong>:
              <ul>
                <li><code>track_processor.image_path</code>: The path to the track image.</li>
                <li><code>track_processor.color_hsv</code>: The color of the track in HSV format.</li>
                <li><code>track_processor.output_image_path_export</code>: The path to save the processed track image.</li>
                <li><code>track_processor.output_npy_path_export</code>: The path to save the processed track numpy file.</li>
                <li><code>track_processor.only_offset_the_outer</code>: Whether to only offset the outer track.</li>
                <li><code>track_processor.dbscan.eps</code>, <code>min_samples</code>, <code>cluster_size_threshold</code>: Parameters for the DBSCAN clustering.</li>
              </ul>
            </li>
            <li>
              <strong>General parameters, <code>actions</code> objects</strong>:
              <ul>
                <li><code>actions.early_stop_after_x_seconds</code>: Seconds to wait before stopping the benchmark (leave empty to disable).</li>
                <li><code>actions.show_live</code>: Show the live video feed.</li>
                <li><code>actions.save_live_to_disk</code>: Save the live video feed to disk.</li>
              </ul>
            </li>
          </ul>

          <hr>

          <h2 class="title is-3">Usage</h2>
          <p>
            Keep in mind all parameters and configurations related to the project are stored in the <code>config.yaml</code> file.
          </p>
          <ol>
            <li>
              Define the keypoints:
              <ol type="a">
                <li>
                  Define the keypoints from the digital graph, specify the <code>keypoints_definer.source_path</code> in the config.yaml file to the digital track image.
                  <pre><code>drivenetbench define-keypoints --config-file config.yaml
</code></pre>
                  <strong>Key commands for defining keypoints:</strong>
                  <ul>
                    <li>Press <code>z</code> to remove the last point.</li>
                    <li>Press <code>q</code> to save the points and exit.</li>
                  </ul>
                </li>
                <li>
                  Change the path of the <code>keypoints_definer.source_path</code> in the config.yaml to your source path. Then re-run the previous command.
                </li>
                <li>
                  Ensure that point order is consistent. You can check your view transformation using the <code>notebooks/calculate_transformation_error.ipynb</code>.
                  <br>
                  Example plots might look like:
                  <figure>
                    <img src="./assets/transformation_error.png" alt="Transformation Error">
                  </figure>
                  Example of keypoints definition on the digital graph:
                  <figure>
                    <img src="./assets/define-keypoints-track.png" alt="Digital Graph Keypoints">
                  </figure>
                  ...and on the camera view:
                  <figure>
                    <img src="./assets/define-keypoints-camera.png" alt="Camera View Keypoints">
                  </figure>
                </li>
              </ol>
            </li>
            <li>
              Extract the track from the image:
              <pre><code>drivenetbench extract-track --config-file config.yaml
</code></pre>
              This produces:
              <ul>
                <li>The track as a .npy file.</li>
                <li>The annotated track image:
                  <figure>
                    <img src="./assets/new_track/annotated_track.jpg" alt="Annotated Track">
                  </figure>
                </li>
              </ul>
            </li>
            <li>
              Run the benchmark:
              <pre><code>drivenetbench benchmark --config-file config.yaml
</code></pre>
              The results (time to complete rotation, path similarity, etc.) are saved in the <code>results</code> directory.
            </li>
          </ol>

          <hr>

          <h2 class="title is-3">Contributing</h2>
          <p>
            We welcome contributions to DriveNetBench! If you have any ideas, suggestions, or bug reports,
            please open an issue on GitHub. If you would like to contribute code, please follow these guidelines:
          </p>
          <ol>
            <li>Fork the repository and create a new branch for your feature or bugfix.</li>
            <li>Write tests for your changes and ensure that all tests pass.</li>
            <li>Submit a pull request with a clear description of your changes.</li>
          </ol>

          <hr>

          <h2 class="title is-3">License</h2>
          <p>
            DriveNetBench is licensed under the <strong>GNU Affero General Public License v3 (AGPL-3.0)</strong>.
            See the <code>LICENSE</code> file for more information.
          </p>

          <hr>

          <h2 class="title is-3">Acknowledgments</h2>
          <p>
            This project incorporates YOLOv11 trained via
            <a href="https://github.com/ultralytics/ultralytics" target="_blank">Ultralytics</a>,
            licensed under the <a href="https://github.com/ultralytics/ultralytics/blob/main/LICENSE" target="_blank">AGPL-3.0</a> license.
          </p>
        </div>

        <!-- END: Copied README content, badges removed -->

      </div>
    </div>
  </div>
</section>
<!-- / Main Content (Readme) -->


<!-- Footer -->
<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- Example link left as in original, remove or replace as desired -->
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>
<!-- / Footer -->

</body>
</html>
